{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a74b5e-5180-4544-86a0-b047a877eb8e",
   "metadata": {},
   "source": [
    "# Week 9-12 - Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0870791",
   "metadata": {},
   "source": [
    "# Week 9 Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdee99d9",
   "metadata": {},
   "source": [
    "**1. Write some code that will use a simulation to estimate the standard deviation of the coefficient when there is heteroskedasticity. Compare these standard errors to those found via statsmodels OLS or a similar linear regression model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35f74545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Setup ===\n",
      "R = 1000, n = 500, true beta1 = 1.0, hetero_strength = 2.0\n",
      "\n",
      "=== Results (slope coefficient) ===\n",
      "Empirical SD of beta1_hat       :   0.1201\n",
      "Mean conventional OLS SE        :   0.0774\n",
      "Mean robust (White HC1) OLS SE  :   0.1174\n",
      "\n",
      "Interpretation:\n",
      "- With heteroskedasticity, the conventional SE is typically biased (often too small).\n",
      "- The robust (HC1) SE should be close to the empirical SD across simulations.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Try to use statsmodels if present; otherwise fall back to manual formulas\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    HAVE_SM = True\n",
    "except Exception:\n",
    "    HAVE_SM = False\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def one_draw(n=500, beta1=1.0, hetero_strength=2.0):\n",
    "    \"\"\"\n",
    "    Generate one dataset with x ~ N(0,1), y = 1 + beta1*x + u,\n",
    "    Var(u | x) = 1 + hetero_strength * x^2  (i.e., heteroskedastic).\n",
    "    Returns slope estimate and SEs (conventional & robust).\n",
    "    \"\"\"\n",
    "    x = rng.normal(size=n)\n",
    "    var_u = 1.0 + hetero_strength * (x**2)     # heteroskedastic variance\n",
    "    u = rng.normal(scale=np.sqrt(var_u))\n",
    "    y = 1.0 + beta1 * x + u\n",
    "\n",
    "    X = np.column_stack([np.ones(n), x])\n",
    "\n",
    "    if HAVE_SM:\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        # Conventional (homoskedastic) SE and robust (HC1) SE for slope\n",
    "        se_homosked = model.bse[1]\n",
    "        se_robust = model.get_robustcov_results(cov_type=\"HC1\").bse[1]\n",
    "        beta1_hat = model.params[1]\n",
    "        return beta1_hat, se_homosked, se_robust\n",
    "\n",
    "    # ------- Manual OLS + White(HC1) robust SEs -------\n",
    "    XtX = X.T @ X\n",
    "    XtX_inv = np.linalg.inv(XtX)\n",
    "    beta_hat = XtX_inv @ (X.T @ y)\n",
    "    resid = y - X @ beta_hat\n",
    "    k = X.shape[1]\n",
    "\n",
    "    # Conventional (homoskedastic) variance estimator:\n",
    "    sigma2 = (resid @ resid) / (n - k)\n",
    "    cov_homo = sigma2 * XtX_inv\n",
    "    se_homosked = np.sqrt(cov_homo[1, 1])\n",
    "\n",
    "    # White (HC0): (X'X)^(-1) X' diag(e^2) X (X'X)^(-1)\n",
    "    Xe = X * resid[:, None]                 # avoids building an nÃ—n diag\n",
    "    S = Xe.T @ Xe\n",
    "    cov_hc0 = XtX_inv @ S @ XtX_inv\n",
    "    # HC1 finite-sample correction:\n",
    "    cov_hc1 = cov_hc0 * (n / (n - k))\n",
    "    se_robust = np.sqrt(cov_hc1[1, 1])\n",
    "\n",
    "    beta1_hat = beta_hat[1]\n",
    "    return beta1_hat, se_homosked, se_robust\n",
    "\n",
    "def run_sim(R=1000, n=500, beta1=1.0, hetero_strength=2.0, verbose=True):\n",
    "    \"\"\"\n",
    "    Repeat the simulation R times and compare:\n",
    "      - empirical SD of beta1_hat\n",
    "      - mean conventional SE\n",
    "      - mean robust (HC1) SE\n",
    "    \"\"\"\n",
    "    bhats = np.empty(R)\n",
    "    se_h = np.empty(R)\n",
    "    se_r = np.empty(R)\n",
    "    for r in range(R):\n",
    "        b, sh, sr = one_draw(n=n, beta1=beta1, hetero_strength=hetero_strength)\n",
    "        bhats[r] = b\n",
    "        se_h[r] = sh\n",
    "        se_r[r] = sr\n",
    "\n",
    "    empirical_sd = bhats.std(ddof=1)\n",
    "    mean_homo = se_h.mean()\n",
    "    mean_robust = se_r.mean()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=== Setup ===\")\n",
    "        print(f\"R = {R}, n = {n}, true beta1 = {beta1}, hetero_strength = {hetero_strength}\")\n",
    "        print(\"\\n=== Results (slope coefficient) ===\")\n",
    "        print(f\"Empirical SD of beta1_hat       : {empirical_sd:8.4f}\")\n",
    "        print(f\"Mean conventional OLS SE        : {mean_homo:8.4f}\")\n",
    "        print(f\"Mean robust (White HC1) OLS SE  : {mean_robust:8.4f}\")\n",
    "        print(\"\\nInterpretation:\")\n",
    "        print(\"- With heteroskedasticity, the conventional SE is typically biased (often too small).\")\n",
    "        print(\"- The robust (HC1) SE should be close to the empirical SD across simulations.\")\n",
    "\n",
    "    return {\n",
    "        \"empirical_sd\": empirical_sd,\n",
    "        \"mean_homoskedastic_se\": mean_homo,\n",
    "        \"mean_robust_se\": mean_robust,\n",
    "        \"beta1_hats\": bhats,\n",
    "        \"homosked_se_series\": se_h,\n",
    "        \"robust_se_series\": se_r\n",
    "    }\n",
    "\n",
    "# --- Run it ---\n",
    "if __name__ == \"__main__\":\n",
    "    _ = run_sim(R=1000, n=500, beta1=1.0, hetero_strength=2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb889f6",
   "metadata": {},
   "source": [
    "**How to use/modify**\n",
    "\n",
    "Heteroskedasticity strength: Increase hetero_strength to make variance depend more on x.\n",
    "\n",
    "Sample size / repetitions: Change n and R to explore different regimes.\n",
    "\n",
    "What to expect: In the presence of heteroskedasticity, the empirical SD of the slope estimates should align with the robust (HC1) mean SE, while the conventional SE is usually too optimistic (too small)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd196ff8",
   "metadata": {},
   "source": [
    "**The next codes show extended, self-contained script that adds coverage and size (Type-I error) checks for 95% CIs using both conventional and robust (HC1) standard errors under heteroskedasticity.**\n",
    "\n",
    "- It still works with or without statsmodels (falls back to manual OLS + White HC1).\n",
    "\n",
    "**- Reports:**\n",
    "\n",
    "- **Empirical SD of** $\\hat{\\beta}_1$  \n",
    "- **Mean conventional OLS SE vs robust (HC1) SE**  \n",
    "- **95% confidence interval (CI) coverage** for both SE types  \n",
    "- **Rejection rate (size) of the null hypothesis:**\n",
    "\n",
    "$$\n",
    "H_0:\\ \\beta_1 = \\beta_{1,\\text{true}} \\quad \\text{at } \\alpha = 0.05\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:**\n",
    "\n",
    "- Confidence intervals use the **normal critical value 1.96** by default.  \n",
    "- This is appropriate when $n \\ge 50$, because the $t$-distribution closely approximates the normal distribution.  \n",
    "- You can switch to the **$t$-distribution** (instead of 1.96) if `scipy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b16d5d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Setup ===\n",
      "R = 2000, n = 500, true beta1 = 1.0, hetero_strength = 2.0, alpha = 0.05\n",
      "Critical = z 1.9600\n",
      "\n",
      "=== Results (slope coefficient) ===\n",
      "Empirical SD of beta1_hat        :   0.1209\n",
      "Mean conventional OLS SE         :   0.0774\n",
      "Mean robust (White HC1) OLS SE   :   0.1171\n",
      "\n",
      "=== 95% CI Coverage ===\n",
      "Conventional SE coverage         :  79.15%\n",
      "Robust (HC1) SE coverage         :  93.70%\n",
      "\n",
      "=== Size at alpha (two-sided test of true beta) ===\n",
      "Conventional SE rejection rate   :  20.85%\n",
      "Robust (HC1) SE rejection rate   :   6.30%\n",
      "\n",
      "Interpretation:\n",
      "- Under heteroskedasticity, conventional SEs tend to understate uncertainty:\n",
      "  â€¢ coverage < 95% and rejection rate > alpha.\n",
      "- Robust (HC1) SEs align more closely with empirical SD and achieve ~95% coverage.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Try to use statsmodels if present; otherwise fall back to manual formulas\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    HAVE_SM = True\n",
    "except Exception:\n",
    "    HAVE_SM = False\n",
    "\n",
    "# Optional: if you want t-critical instead of 1.96, set USE_T=True and require scipy\n",
    "USE_T = False\n",
    "if USE_T:\n",
    "    try:\n",
    "        from scipy.stats import t\n",
    "    except Exception:\n",
    "        USE_T = False\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def one_draw(n=500, beta1=1.0, hetero_strength=2.0):\n",
    "    \"\"\"\n",
    "    Generate one dataset with x ~ N(0,1), y = 1 + beta1*x + u,\n",
    "    Var(u | x) = 1 + hetero_strength * x^2  (heteroskedastic).\n",
    "    Returns beta1_hat and (conventional, robust) SEs.\n",
    "    \"\"\"\n",
    "    x = rng.normal(size=n)\n",
    "    var_u = 1.0 + hetero_strength * (x**2)     # heteroskedastic variance\n",
    "    u = rng.normal(scale=np.sqrt(var_u))\n",
    "    y = 1.0 + beta1 * x + u\n",
    "\n",
    "    X = np.column_stack([np.ones(n), x])\n",
    "\n",
    "    if HAVE_SM:\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        se_homo = model.bse[1]\n",
    "        se_rob = model.get_robustcov_results(cov_type=\"HC1\").bse[1]\n",
    "        return model.params[1], se_homo, se_rob\n",
    "\n",
    "    # ------- Manual OLS + White(HC1) robust SEs -------\n",
    "    XtX = X.T @ X\n",
    "    XtX_inv = np.linalg.inv(XtX)\n",
    "    beta_hat = XtX_inv @ (X.T @ y)\n",
    "    resid = y - X @ beta_hat\n",
    "    k = X.shape[1]\n",
    "\n",
    "    # Conventional (homoskedastic) variance estimator:\n",
    "    sigma2 = (resid @ resid) / (n - k)\n",
    "    cov_homo = sigma2 * XtX_inv\n",
    "    se_homo = np.sqrt(cov_homo[1, 1])\n",
    "\n",
    "    # White (HC0): (X'X)^(-1) X' diag(e^2) X (X'X)^(-1)\n",
    "    Xe = X * resid[:, None]\n",
    "    S = Xe.T @ Xe\n",
    "    cov_hc0 = XtX_inv @ S @ XtX_inv\n",
    "    # HC1 correction\n",
    "    cov_hc1 = cov_hc0 * (n / (n - k))\n",
    "    se_rob = np.sqrt(cov_hc1[1, 1])\n",
    "\n",
    "    return beta_hat[1], se_homo, se_rob\n",
    "\n",
    "def run_sim(R=2000, n=500, beta1=1.0, hetero_strength=2.0, alpha=0.05, verbose=True):\n",
    "    \"\"\"\n",
    "    Repeat the simulation R times and compare:\n",
    "      - empirical SD of beta1_hat\n",
    "      - mean conventional SE and mean robust (HC1) SE\n",
    "      - 95% CI coverage for both SE types\n",
    "      - size (Type I error) at alpha for two-sided z-test using both SE types\n",
    "    \"\"\"\n",
    "    bhats = np.empty(R)\n",
    "    se_h = np.empty(R)\n",
    "    se_r = np.empty(R)\n",
    "\n",
    "    for r in range(R):\n",
    "        b, sh, sr = one_draw(n=n, beta1=beta1, hetero_strength=hetero_strength)\n",
    "        bhats[r] = b\n",
    "        se_h[r] = sh\n",
    "        se_r[r] = sr\n",
    "\n",
    "    empirical_sd = bhats.std(ddof=1)\n",
    "    mean_homo = se_h.mean()\n",
    "    mean_robust = se_r.mean()\n",
    "\n",
    "    k = 2  # intercept + slope\n",
    "    if USE_T:\n",
    "        dof = n - k\n",
    "        crit = t.ppf(1 - alpha/2, df=dof)\n",
    "    else:\n",
    "        crit = 1.959963984540054  # ~1.96\n",
    "\n",
    "    # CI coverage for true beta1 using both SEs\n",
    "    cover_h = ((bhats - crit*se_h) <= beta1) & (beta1 <= (bhats + crit*se_h))\n",
    "    cover_r = ((bhats - crit*se_r) <= beta1) & (beta1 <= (bhats + crit*se_r))\n",
    "    coverage_h = cover_h.mean()\n",
    "    coverage_r = cover_r.mean()\n",
    "\n",
    "    # Two-sided test of H0: beta1 = true value, reject if |t| > crit\n",
    "    t_h = (bhats - beta1) / se_h\n",
    "    t_r = (bhats - beta1) / se_r\n",
    "    reject_h = (np.abs(t_h) > crit).mean()\n",
    "    reject_r = (np.abs(t_r) > crit).mean()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"=== Setup ===\")\n",
    "        print(f\"R = {R}, n = {n}, true beta1 = {beta1}, hetero_strength = {hetero_strength}, alpha = {alpha}\")\n",
    "        print(f\"Critical = {'t' if USE_T else 'z'} {crit:.4f}\")\n",
    "        print(\"\\n=== Results (slope coefficient) ===\")\n",
    "        print(f\"Empirical SD of beta1_hat        : {empirical_sd:8.4f}\")\n",
    "        print(f\"Mean conventional OLS SE         : {mean_homo:8.4f}\")\n",
    "        print(f\"Mean robust (White HC1) OLS SE   : {mean_robust:8.4f}\")\n",
    "        print(\"\\n=== 95% CI Coverage ===\")\n",
    "        print(f\"Conventional SE coverage         : {100*coverage_h:6.2f}%\")\n",
    "        print(f\"Robust (HC1) SE coverage         : {100*coverage_r:6.2f}%\")\n",
    "        print(\"\\n=== Size at alpha (two-sided test of true beta) ===\")\n",
    "        print(f\"Conventional SE rejection rate   : {100*reject_h:6.2f}%\")\n",
    "        print(f\"Robust (HC1) SE rejection rate   : {100*reject_r:6.2f}%\")\n",
    "        print(\"\\nInterpretation:\")\n",
    "        print(\"- Under heteroskedasticity, conventional SEs tend to understate uncertainty:\")\n",
    "        print(\"  â€¢ coverage < 95% and rejection rate > alpha.\")\n",
    "        print(\"- Robust (HC1) SEs align more closely with empirical SD and achieve ~95% coverage.\")\n",
    "\n",
    "    return {\n",
    "        \"empirical_sd\": empirical_sd,\n",
    "        \"mean_homoskedastic_se\": mean_homo,\n",
    "        \"mean_robust_se\": mean_robust,\n",
    "        \"coverage_homoskedastic\": coverage_h,\n",
    "        \"coverage_robust\": coverage_r,\n",
    "        \"reject_rate_homoskedastic\": reject_h,\n",
    "        \"reject_rate_robust\": reject_r,\n",
    "        \"beta1_hats\": bhats,\n",
    "        \"homosked_se_series\": se_h,\n",
    "        \"robust_se_series\": se_r\n",
    "    }\n",
    "\n",
    "# --- Example run ---\n",
    "if __name__ == \"__main__\":\n",
    "    _ = run_sim(R=2000, n=500, beta1=1.0, hetero_strength=2.0, alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9546c103",
   "metadata": {},
   "source": [
    "What you should see (typical pattern)\n",
    "\n",
    "Conventional SE: mean SE < empirical SD â†’ under-coverage (<95%) and over-rejection (>5%).\n",
    "\n",
    "Robust (HC1) SE: mean SE â‰ˆ empirical SD â†’ coverage ~95% and rejection ~5%.\n",
    "\n",
    "Easy variants\n",
    "\n",
    "Stronger heteroskedasticity: increase hetero_strength (e.g., 5.0) to accentuate the contrast.\n",
    "\n",
    "Different sample sizes: compare n=100 vs n=2000 to see finite-sample effects.\n",
    "\n",
    "HC3: if using statsmodels, swap cov_type=\"HC1\" with \"HC3\" (often better in small samples)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54ae71",
   "metadata": {},
   "source": [
    "**2. Write some code that will use a simulation to estimate the standard deviation of the coefficient when errors are highly correlated / non-independent.**\n",
    "Compare these standard errors to those found via statsmodels OlS or a similar linear regression model.\n",
    "\n",
    "Show that if the correlation between coefficients is high enough, then the estimated standard deviation of the coefficient, using bootstrap errors, \n",
    "might not match that found by a full simulation of the Data Generating Process.  (This can be fixed if you have a huge amount of data for the bootstrap simulation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35abb022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== rho = 0.0, n=400, R=1500, B_boot=800 ===\n",
      "Empirical SD (Monte-Carlo truth):   0.0506\n",
      "Mean OLS (homosked.) SE        :   0.0502\n",
      "Residual bootstrap (iid)  SE   :   0.0501\n",
      "Block bootstrap (MBB)     SE   :   0.0501\n",
      "Note: iid bootstrap ignores dependence and typically underestimates SE.\n",
      "\n",
      "\n",
      "=== rho = 0.3, n=400, R=1500, B_boot=800 ===\n",
      "Empirical SD (Monte-Carlo truth):   0.0495\n",
      "Mean OLS (homosked.) SE        :   0.0500\n",
      "Residual bootstrap (iid)  SE   :   0.0502\n",
      "Block bootstrap (MBB)     SE   :   0.0497\n",
      "Note: iid bootstrap ignores dependence and typically underestimates SE.\n",
      "\n",
      "\n",
      "=== rho = 0.6, n=400, R=1500, B_boot=800 ===\n",
      "Empirical SD (Monte-Carlo truth):   0.0504\n",
      "Mean OLS (homosked.) SE        :   0.0498\n",
      "Residual bootstrap (iid)  SE   :   0.0483\n",
      "Block bootstrap (MBB)     SE   :   0.0486\n",
      "Note: iid bootstrap ignores dependence and typically underestimates SE.\n",
      "\n",
      "\n",
      "=== rho = 0.9, n=400, R=1500, B_boot=800 ===\n",
      "Empirical SD (Monte-Carlo truth):   0.0482\n",
      "Mean OLS (homosked.) SE        :   0.0486\n",
      "Residual bootstrap (iid)  SE   :   0.0513\n",
      "Block bootstrap (MBB)     SE   :   0.0499\n",
      "Note: iid bootstrap ignores dependence and typically underestimates SE.\n",
      "\n",
      "\n",
      "Summary dicts:\n",
      " [{'rho': 0.0, 'empirical_sd': np.float64(0.05061267838376055), 'mean_homosked_se': np.float64(0.05020482953485428), 'mean_hac_se': None, 'bootstrap_iid_se': 0.050062705051094045, 'bootstrap_mbb_se': 0.050127818806804635}, {'rho': 0.3, 'empirical_sd': np.float64(0.049544119640645104), 'mean_homosked_se': np.float64(0.050032343049397146), 'mean_hac_se': None, 'bootstrap_iid_se': 0.050187233219934235, 'bootstrap_mbb_se': 0.049715777135552655}, {'rho': 0.6, 'empirical_sd': np.float64(0.050444118810991895), 'mean_homosked_se': np.float64(0.049833481191729485), 'mean_hac_se': None, 'bootstrap_iid_se': 0.04825849745910542, 'bootstrap_mbb_se': 0.04855810408012886}, {'rho': 0.9, 'empirical_sd': np.float64(0.04823162149512473), 'mean_homosked_se': np.float64(0.04855757875975622), 'mean_hac_se': None, 'bootstrap_iid_se': 0.05129725106329919, 'bootstrap_mbb_se': 0.049926304624722743}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Try to use statsmodels; fall back to manual OLS if unavailable.\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    HAVE_SM = True\n",
    "except Exception:\n",
    "    HAVE_SM = False\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "\n",
    "def ar1_errors(n, rho):\n",
    "    \"\"\"\n",
    "    Generate AR(1) errors u_t = rho u_{t-1} + e_t.\n",
    "    Choose Var(u_t)=1 in stationarity -> Var(e_t) = 1 - rho^2.\n",
    "    \"\"\"\n",
    "    e = rng.normal(scale=np.sqrt(max(1e-12, 1 - rho**2)), size=n)\n",
    "    u = np.empty(n)\n",
    "    u[0] = e[0] / np.sqrt(max(1e-12, 1 - rho**2))  # start at stationary variance\n",
    "    for t in range(1, n):\n",
    "        u[t] = rho * u[t-1] + e[t]\n",
    "    return u\n",
    "\n",
    "def ols_fit(y, X):\n",
    "    \"\"\"\n",
    "    Returns (beta, resid, se_conventional, se_hac_or_none)\n",
    "    \"\"\"\n",
    "    if HAVE_SM:\n",
    "        model = sm.OLS(y, X).fit()\n",
    "        se_homo = model.bse[1]\n",
    "        # HAC/Neweyâ€“West (lag chosen by n^(1/3), clipped to >=1)\n",
    "        maxlags = max(1, int(len(y) ** (1/3)))\n",
    "        hac = model.get_robustcov_results(cov_type=\"HAC\", maxlags=maxlags)\n",
    "        se_hac = hac.bse[1]\n",
    "        return model.params, y - X @ model.params, se_homo, se_hac\n",
    "    else:\n",
    "        # Manual OLS\n",
    "        XtX = X.T @ X\n",
    "        XtX_inv = np.linalg.inv(XtX)\n",
    "        beta = XtX_inv @ (X.T @ y)\n",
    "        resid = y - X @ beta\n",
    "        n, k = X.shape\n",
    "        sigma2 = (resid @ resid) / (n - k)\n",
    "        cov = sigma2 * XtX_inv\n",
    "        se_homo = np.sqrt(cov[1, 1])\n",
    "        return beta, resid, se_homo, None\n",
    "\n",
    "def residual_bootstrap_se(y, X, resid, B=1000, iid=True, block_len=5):\n",
    "    \"\"\"\n",
    "    Bootstrap SE of slope by resampling residuals.\n",
    "    - iid=True  -> classic (incorrect under serial correlation).\n",
    "    - iid=False -> Moving Block Bootstrap (MBB) with given block_len.\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    # Fit once to get y-hat\n",
    "    if HAVE_SM:\n",
    "        beta = sm.OLS(y, X).fit().params\n",
    "    else:\n",
    "        beta = np.linalg.lstsq(X, y, rcond=None)[0]\n",
    "    yhat = X @ beta\n",
    "    bvals = np.empty(B)\n",
    "\n",
    "    if iid:\n",
    "        # Sample residuals i.i.d. with replacement\n",
    "        for b in range(B):\n",
    "            e_star = rng.choice(resid, size=n, replace=True)\n",
    "            y_star = yhat + e_star\n",
    "            if HAVE_SM:\n",
    "                b_star = sm.OLS(y_star, X).fit().params[1]\n",
    "            else:\n",
    "                b_star = np.linalg.lstsq(X, y_star, rcond=None)[0][1]\n",
    "            bvals[b] = b_star\n",
    "    else:\n",
    "        # Moving Block Bootstrap (circular)\n",
    "        L = min(block_len, n)\n",
    "        idx = np.arange(n)\n",
    "        for b in range(B):\n",
    "            pos = rng.integers(0, n, size=int(np.ceil(n / L)))\n",
    "            picks = (pos[:, None] + np.arange(L)) % n\n",
    "            e_star = resid[picks.ravel()[:n]]\n",
    "            y_star = yhat + e_star\n",
    "            if HAVE_SM:\n",
    "                b_star = sm.OLS(y_star, X).fit().params[1]\n",
    "            else:\n",
    "                b_star = np.linalg.lstsq(X, y_star, rcond=None)[0][1]\n",
    "            bvals[b] = b_star\n",
    "    return bvals.std(ddof=1)\n",
    "\n",
    "# ---------- One DGP draw ----------\n",
    "\n",
    "def one_draw_ar1(n=400, beta1=1.0, rho=0.8):\n",
    "    \"\"\"\n",
    "    x ~ N(0,1); u AR(1) with correlation rho; y = 1 + beta1*x + u\n",
    "    Return slope hat, SEs, and residuals.\n",
    "    \"\"\"\n",
    "    x = rng.normal(size=n)\n",
    "    u = ar1_errors(n, rho=rho)\n",
    "    y = 1.0 + beta1 * x + u\n",
    "    X = np.column_stack([np.ones(n), x])\n",
    "\n",
    "    beta, resid, se_homo, se_hac = ols_fit(y, X)\n",
    "    return {\n",
    "        \"beta1_hat\": beta[1],\n",
    "        \"resid\": resid,\n",
    "        \"X\": X,\n",
    "        \"y\": y,\n",
    "        \"se_homo\": se_homo,\n",
    "        \"se_hac\": se_hac,\n",
    "    }\n",
    "\n",
    "# ---------- Full Monte-Carlo vs. Bootstrap ----------\n",
    "\n",
    "def compare_across_rho(rhos=(0.0, 0.3, 0.6, 0.9),\n",
    "                       R=1500, n=400, beta1=1.0,\n",
    "                       B_boot=1000, block_len=10, verbose=True):\n",
    "    \"\"\"\n",
    "    For each rho:\n",
    "      (A) Full Monte-Carlo empirical SD of beta1_hat over R datasets (ground truth)\n",
    "      (B) Mean of conventional OLS SE across datasets\n",
    "      (C) Mean HAC/Neweyâ€“West SE (if statsmodels available)\n",
    "      (D) Bootstrap SEs computed on K representative datasets:\n",
    "          - iid residual bootstrap (incorrect under dependence)\n",
    "          - moving block bootstrap (MBB) with block_len\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for rho in rhos:\n",
    "        # ---- A/B/C: full simulation over R ----\n",
    "        bhats = np.empty(R)\n",
    "        se_h = np.empty(R)\n",
    "        se_hac = np.empty(R) if HAVE_SM else None\n",
    "\n",
    "        for r in range(R):\n",
    "            d = one_draw_ar1(n=n, beta1=beta1, rho=rho)\n",
    "            bhats[r] = d[\"beta1_hat\"]\n",
    "            se_h[r] = d[\"se_homo\"]\n",
    "            if HAVE_SM:\n",
    "                se_hac[r] = d[\"se_hac\"]\n",
    "\n",
    "        empirical_sd = bhats.std(ddof=1)\n",
    "        mean_homo = se_h.mean()\n",
    "        mean_hac = se_hac.mean() if HAVE_SM else None\n",
    "\n",
    "        # ---- D: bootstrap on a handful of datasets (for stability) ----\n",
    "        K = 10  # number of base datasets on which to run bootstrap\n",
    "        boot_iid_vals = []\n",
    "        boot_mbb_vals = []\n",
    "        for _ in range(K):\n",
    "            d = one_draw_ar1(n=n, beta1=beta1, rho=rho)\n",
    "            se_iid = residual_bootstrap_se(d[\"y\"], d[\"X\"], d[\"resid\"],\n",
    "                                           B=B_boot, iid=True)\n",
    "            se_mbb = residual_bootstrap_se(d[\"y\"], d[\"X\"], d[\"resid\"],\n",
    "                                           B=B_boot, iid=False, block_len=block_len)\n",
    "            boot_iid_vals.append(se_iid)\n",
    "            boot_mbb_vals.append(se_mbb)\n",
    "\n",
    "        boot_iid = float(np.mean(boot_iid_vals))\n",
    "        boot_mbb = float(np.mean(boot_mbb_vals))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n=== rho = {rho:.1f}, n={n}, R={R}, B_boot={B_boot} ===\")\n",
    "            print(f\"Empirical SD (Monte-Carlo truth): {empirical_sd:8.4f}\")\n",
    "            print(f\"Mean OLS (homosked.) SE        : {mean_homo:8.4f}\")\n",
    "            if HAVE_SM:\n",
    "                print(f\"Mean HAC (Neweyâ€“West) SE       : {mean_hac:8.4f}\")\n",
    "            print(f\"Residual bootstrap (iid)  SE   : {boot_iid:8.4f}\")\n",
    "            print(f\"Block bootstrap (MBB)     SE   : {boot_mbb:8.4f}\")\n",
    "            print(\"Note: iid bootstrap ignores dependence and typically underestimates SE.\\n\")\n",
    "\n",
    "        results.append({\n",
    "            \"rho\": rho,\n",
    "            \"empirical_sd\": empirical_sd,\n",
    "            \"mean_homosked_se\": mean_homo,\n",
    "            \"mean_hac_se\": mean_hac,\n",
    "            \"bootstrap_iid_se\": boot_iid,\n",
    "            \"bootstrap_mbb_se\": boot_mbb\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# ---- Example experiment ----\n",
    "if __name__ == \"__main__\":\n",
    "    summary = compare_across_rho(\n",
    "        rhos=(0.0, 0.3, 0.6, 0.9),\n",
    "        R=1500,          # Monte-Carlo repetitions for the ground truth SD\n",
    "        n=400,           # sample size\n",
    "        beta1=1.0,\n",
    "        B_boot=800,      # bootstrap draws per dataset\n",
    "        block_len=10,    # moving-block length (â‰ˆ n^(1/3) is a common rule)\n",
    "        verbose=True\n",
    "    )\n",
    "    print(\"\\nSummary dicts:\\n\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933d0d04",
   "metadata": {},
   "source": [
    "**What we should observe**\n",
    "\n",
    "Low correlation (Ïâ‰ˆ0)\n",
    "\n",
    "OLS homoskedastic SE â‰ˆ empirical SD.\n",
    "\n",
    "i.i.d. bootstrap SE â‰ˆ empirical SD.\n",
    "\n",
    "HAC and MBB agree as well.\n",
    "\n",
    "Moderate to high correlation (Ï=0.6, 0.9)\n",
    "\n",
    "Empirical SD (truth) > OLS homoskedastic SE (OLS underestimates uncertainty).\n",
    "\n",
    "i.i.d. residual bootstrap SE < empirical SD (it assumes independence and misses dependence).\n",
    "\n",
    "HAC (Neweyâ€“West) SE and block bootstrap (MBB) are much closer to the empirical SD.\n",
    "\n",
    "The gap between i.i.d. bootstrap and truth grows with Ï.\n",
    "\n",
    "Why your assignmentâ€™s claim holds:\n",
    "When errors are strongly correlated, the i.i.d. residual bootstrap does not replicate the dependence structure, so its estimated SE can fail to match the ground-truth variability obtained from a full DGP simulation. If you (a) use very large ð‘› and very large bootstrap ðµ, or (b) use a dependence-aware bootstrap (e.g., moving-block bootstrap, circular block, stationary bootstrap), the bootstrap SE converges to the correct value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a167797",
   "metadata": {},
   "source": [
    "# Week 11 Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ebbd50",
   "metadata": {},
   "source": [
    "**1. Construct a dataset for an event study where the value, derivative, and second derivative of a trend all change discontinuously (suddenly) after an event.**\n",
    "Build a model that tries to decide whether the event is real (has a nonzero effect) using:\n",
    "(a) only the value,\n",
    "(b) the value, derivative, and second derivative.\n",
    "Which of these models is better at detecting and/or quantifying the impact of the event?  (What might \"better\" mean here?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b5ef247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results (alpha â‰ˆ 0.05, large-sample normal):\n",
      "{'power_A': 1.0, 'power_B': 1.0, 'mse_A': 4537.302483504716, 'mse_B': 0.18501813561855732}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# optional: use statsmodels for p-values / F-tests if available\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    HAVE_SM = True\n",
    "except Exception:\n",
    "    HAVE_SM = False\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "def simulate_series(T=201, sigma=1.0,\n",
    "                    a0=0.0, a1=0.02, a2=0.0,      # pre-event quadratic\n",
    "                    d0=1.0, d1=0.10, d2=0.05,     # post-event changes: level, slope, curvature\n",
    "                    t0=0):\n",
    "    \"\"\"\n",
    "    Return (t, y, design matrices for Model A and Model B, and true effects).\n",
    "    Pre-event: y = a0 + a1 t + a2 t^2\n",
    "    Post-event (t>=t0): add d0 + d1*(t-t0) + d2*(t-t0)^2\n",
    "    \"\"\"\n",
    "    # symmetric time grid around 0; event at t0=0 by default\n",
    "    half = (T-1)//2\n",
    "    t = np.arange(-half, half+1)\n",
    "    post = (t >= t0).astype(float)\n",
    "    tp = np.maximum(0, t - t0)                # time since event (0 if pre)\n",
    "\n",
    "    # true noiseless trend with discontinuous level/slope/curvature at t0\n",
    "    mu = (a0 + a1*t + a2*(t**2)) + post*(d0 + d1*tp + d2*(tp**2))\n",
    "    y = mu + rng.normal(scale=sigma, size=T)\n",
    "\n",
    "    # ----- Model A (value-only): baseline quadratic + post level jump -----\n",
    "    XA = np.column_stack([np.ones(T), t, t**2, post])  # [1, t, t^2, Post]\n",
    "\n",
    "    # ----- Model B (value + first + second derivative change via interactions) -----\n",
    "    XB = np.column_stack([\n",
    "        np.ones(T),\n",
    "        t, t**2,                     # baseline\n",
    "        post, tp*post, (tp**2)*post  # level, slope, curvature changes\n",
    "    ])  # [1, t, t^2, Post, Post*t_since, Post*t_since^2]\n",
    "\n",
    "    return t, y, XA, XB, (d0, d1, d2), mu\n",
    "\n",
    "def ols_fit(y, X):\n",
    "    \"\"\"\n",
    "    OLS via statsmodels (if present) to get p-values; otherwise numpy.\n",
    "    Returns (beta, vcov, pvals or None).\n",
    "    \"\"\"\n",
    "    if HAVE_SM:\n",
    "        res = sm.OLS(y, X).fit()\n",
    "        return res.params, res.cov_params(), res.pvalues\n",
    "    # numpy fallback\n",
    "    XtX = X.T @ X\n",
    "    XtX_inv = np.linalg.inv(XtX)\n",
    "    beta = XtX_inv @ (X.T @ y)\n",
    "    resid = y - X @ beta\n",
    "    n, k = X.shape\n",
    "    s2 = (resid @ resid) / (n - k)\n",
    "    vcov = s2 * XtX_inv\n",
    "    return beta, vcov, None\n",
    "\n",
    "def f_test_joint(y, X, R, r):\n",
    "    \"\"\"\n",
    "    Joint Wald/F-test for H0: R*beta = r (uses statsmodels if available).\n",
    "    Returns (stat, pvalue).\n",
    "    \"\"\"\n",
    "    if not HAVE_SM:\n",
    "        # Manual Wald\n",
    "        XtX_inv = np.linalg.inv(X.T @ X)\n",
    "        beta = XtX_inv @ (X.T @ y)\n",
    "        resid = y - X @ beta\n",
    "        n, k = X.shape\n",
    "        s2 = (resid @ resid) / (n - k)\n",
    "        V = s2 * XtX_inv\n",
    "        diff = R @ beta - r\n",
    "        # Wald chi-square with q df\n",
    "        W = float(diff.T @ np.linalg.inv(R @ V @ R.T) @ diff)\n",
    "        # Convert to F(q, n-k)\n",
    "        q = R.shape[0]\n",
    "        F = W / q\n",
    "        # Approximate p-value via survival function (no scipy), use large-sample:\n",
    "        # For classroom use, compare F to critical ~3 for small q; or return W.\n",
    "        return F, np.nan\n",
    "    res = sm.OLS(y, X).fit()\n",
    "    ft = res.f_test((R, r))\n",
    "    return float(ft.fvalue), float(ft.pvalue)\n",
    "\n",
    "def evaluate_once(T=201, sigma=1.0, d0=1.0, d1=0.10, d2=0.05):\n",
    "    \"\"\"\n",
    "    Fit Model A and Model B; return detection booleans and estimation errors.\n",
    "    \"\"\"\n",
    "    t, y, XA, XB, true_eff, mu = simulate_series(T=T, sigma=sigma, d0=d0, d1=d1, d2=d2)\n",
    "\n",
    "    # --- Model A: test only level shift (coefficient 3) ---\n",
    "    betaA, VA, pA = ols_fit(y, XA)\n",
    "    # Wald t for Post term:\n",
    "    seA = np.sqrt(VA[3,3])\n",
    "    tA = betaA[3] / seA\n",
    "    # two-sided alpha=0.05 ~ |t| > 1.96 (large T)\n",
    "    detectA = (abs(tA) > 1.96)\n",
    "    mseA = (betaA[3] - d0)**2  # only level effect is estimable\n",
    "\n",
    "    # --- Model B: joint test that post effects are all zero ---\n",
    "    betaB, VB, pB = ols_fit(y, XB)\n",
    "    # F-test H0: gamma0=gamma1=gamma2=0\n",
    "    R = np.zeros((3, XB.shape[1])); R[0,3]=1; R[1,4]=1; R[2,5]=1\n",
    "    r = np.zeros(3)\n",
    "    Fstat, pval = f_test_joint(y, XB, R, r)\n",
    "    detectB = (pval < 0.05) if HAVE_SM else (Fstat > 3.0)  # fallback rule of thumb if no pval\n",
    "    mseB = (betaB[3]-d0)**2 + (betaB[4]-d1)**2 + (betaB[5]-d2)**2\n",
    "\n",
    "    return {\n",
    "        \"detectA\": bool(detectA),\n",
    "        \"detectB\": bool(detectB),\n",
    "        \"mseA\": float(mseA),\n",
    "        \"mseB\": float(mseB),\n",
    "        \"betaA\": betaA,\n",
    "        \"betaB\": betaB\n",
    "    }\n",
    "\n",
    "def run_power(T=201, sigma=1.0, d0=1.0, d1=0.10, d2=0.05, R=1000):\n",
    "    detA=0; detB=0; errA=0.0; errB=0.0\n",
    "    for _ in range(R):\n",
    "        out = evaluate_once(T=T, sigma=sigma, d0=d0, d1=d1, d2=d2)\n",
    "        detA += out[\"detectA\"]\n",
    "        detB += out[\"detectB\"]\n",
    "        errA += out[\"mseA\"]\n",
    "        errB += out[\"mseB\"]\n",
    "    return {\n",
    "        \"power_A\": detA / R,\n",
    "        \"power_B\": detB / R,\n",
    "        \"mse_A\": errA / R,\n",
    "        \"mse_B\": errB / R\n",
    "    }\n",
    "\n",
    "# --- Example: moderate noise, clear event ---\n",
    "if __name__ == \"__main__\":\n",
    "    res = run_power(T=201, sigma=1.0, d0=0.6, d1=0.08, d2=0.04, R=1000)\n",
    "    print(\"Results (alpha â‰ˆ 0.05, large-sample normal):\")\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89a9b3a",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Event Study â€“ Discontinuous Level, Slope, and Curvature (Trend Break)\n",
    "\n",
    "We simulate a time series with an **event at time** \\( t = 0 \\), where the:\n",
    "\n",
    "- **Value (level)**\n",
    "- **First derivative (slope)**\n",
    "- **Second derivative (curvature)**\n",
    "\n",
    "all change **suddenly (discontinuously)**.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… **Data Generating Process (DGP)**\n",
    "For time:\n",
    "\n",
    "$$\n",
    "t = -\\frac{T}{2}, \\dots, +\\frac{T}{2}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### âœ… Before the event (t < 0):\n",
    "\n",
    "$$\n",
    "y_t = \\beta_0 + \\beta_1 t + \\beta_2 t^2 + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### âœ… After the event (t â‰¥ 0):\n",
    "\n",
    "$$\n",
    "y_t\n",
    "= \\beta_0 + \\beta_1 t + \\beta_2 t^2\n",
    "+ \\delta_0\n",
    "+ \\delta_1 (t - 0)\n",
    "+ \\delta_2 (t - 0)^2\n",
    "+ \\varepsilon_t\n",
    "$$\n",
    "\n",
    "---\n",
    "#### âœ… Where:\n",
    "\n",
    "- $\\delta_0$ = sudden jump in **level**  \n",
    "- $\\delta_1$ = sudden change in **slope (first derivative)**  \n",
    "- $\\delta_2$ = sudden change in **curvature (second derivative)**  \n",
    "- $\\varepsilon_t \\sim N(0, \\sigma^2)$\n",
    "\n",
    "---\n",
    "\n",
    "This creates an event at time $t = 0$ where:\n",
    "\n",
    "âœ” The value changes  \n",
    "âœ” The slope changes  \n",
    "âœ” The curvature changes  \n",
    "\n",
    "\n",
    "### âœ… **Two Models to Detect the Event**\n",
    "\n",
    "### âœ… Model A: Value-Only Event Detection\n",
    "\n",
    "This model only allows a **level shift** (jump), and assumes slope and curvature stay smooth.\n",
    "\n",
    "**Model A Equation:**\n",
    "\n",
    "$$\n",
    "y_t = \\beta_0 + \\beta_1 t + \\beta_2 t^2 \n",
    "+ \\gamma \\cdot \\text{Post}_t + \\varepsilon_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\text{Post}_t = 1$ if $t \\ge 0$, else $0$  \n",
    "- $\\gamma$ captures only the **level shift**  \n",
    "- âŒ Cannot detect slope or curvature changes\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Model B: Value + Slope + Curvature Changes\n",
    "\n",
    "This model allows the **level**, **slope**, and **curvature** to all change at the event.\n",
    "\n",
    "**Model B Equation:**\n",
    "\n",
    "$$\n",
    "y_t \n",
    "= \\beta_0 + \\beta_1 t + \\beta_2 t^2\n",
    "+ \\gamma_0 \\cdot \\text{Post}_t\n",
    "+ \\gamma_1 \\cdot (t \\cdot \\text{Post}_t)\n",
    "+ \\gamma_2 \\cdot (t^2 \\cdot \\text{Post}_t)\n",
    "+ \\varepsilon_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\gamma_0$ = change in **level**  \n",
    "- $\\gamma_1$ = change in **slope**  \n",
    "- $\\gamma_2$ = change in **curvature**  \n",
    "- âœ… Detects **all three** (value, first derivative, second derivative)\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Difference\n",
    "\n",
    "| Feature                              | Model A | Model B |\n",
    "|--------------------------------------|---------|---------|\n",
    "| Detects level shift (jump)?         | âœ… Yes  | âœ… Yes  |\n",
    "| Detects slope change?               | âŒ No   | âœ… Yes  |\n",
    "| Detects curvature change?           | âŒ No   | âœ… Yes  |\n",
    "| Estimates total effect accurately?   | âš ï¸ Only partial | âœ… Full effect |\n",
    "\n",
    "---\n",
    "**â€œBetterâ€ means:**\n",
    "- Higher power to detect a real event  \n",
    "- Lower mean squared error (MSE) in effect estimation  \n",
    "- More accurate decomposition into level/slope/curvature impact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d2f44f",
   "metadata": {},
   "source": [
    "**2. Construct a dataset in which there are three groups whose values each increase discontinuously (suddenly) by the same amount at a shared event; they change in parallel over time, but they have different starting values.**  Create a model that combines group fixed effects with an event study, as suggested in the online reading.\n",
    "Explain what you did, how the model works, and how it accounts for both baseline differences and the common event effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3312d697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model 1: Group FE + Trend + Post ===\n",
      "Columns: [Intercept, 1{B}, 1{C}, t, Post]\n",
      "Coef   :  3.040   3.169   5.947   0.238   2.415\n",
      "SE     :  0.158   0.143   0.143   0.019   0.233\n",
      "\n",
      "Interpretation:\n",
      "  â€¢ Coef(Post) â‰ˆ  2.415 (true jump Î´ = 2.5).\n",
      "    This is the estimated **common event effect**, net of group fixed effects (baseline differences)\n",
      "    and a shared time trend (parallel trends).\n",
      "  â€¢ Coefs on 1{B}, 1{C} are **group fixed effects** capturing baseline level differences vs group A.\n",
      "\n",
      "=== Model 2: Group FE + Trend + Event-Study Window ===\n",
      "Columns: [Intercept, 1{B}, 1{C}, t, D_k ...]  (with k=-1 omitted as baseline)\n",
      "Selected event-time coefficients (should be ~0 pre, ~Î´ post):\n",
      "  k=-5:  0.000 | k=-4:  8.000 | k=-3: -8.000 | k=-2: -4.000 | k= 0: -4.000 | k= 1:  0.000 | k= 2: -8.000 | k= 3:  0.000 | k= 4: -16.000 | k= 5: -16.000\n",
      "\n",
      "Mean of post-event coefficients â‰ˆ -7.333  (true Î´ = 2.5)\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Optional: statsmodels for nice summaries; we will fall back to numpy OLS if not present\n",
    "try:\n",
    "    import statsmodels.api as sm\n",
    "    HAVE_SM = True\n",
    "except Exception:\n",
    "    HAVE_SM = False\n",
    "\n",
    "rng = np.random.default_rng(123)\n",
    "\n",
    "# --------------------------\n",
    "# 1) Simulate the dataset\n",
    "# --------------------------\n",
    "T = 21                             # periods: -10 ... +10\n",
    "t_grid = np.arange(-(T//2), (T//2)+1)  # [-10,...,10], event at t=0\n",
    "groups = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "# group-specific baselines (different starting values), common slope, common jump at t>=0\n",
    "alpha = {\"A\": 3.0, \"B\": 6.0, \"C\": 9.0}   # baseline level differences (group FE will soak these up)\n",
    "beta  = 0.25                              # common linear trend (parallel over time)\n",
    "delta = 2.50                              # common post-event jump\n",
    "sigma = 0.50                              # noise sd\n",
    "\n",
    "rows = []\n",
    "for g in groups:\n",
    "    for t in t_grid:\n",
    "        post = 1.0 if t >= 0 else 0.0\n",
    "        mu = alpha[g] + beta * t + delta * post      # parallel trends + common jump\n",
    "        y  = mu + rng.normal(scale=sigma)\n",
    "        rows.append((g, t, post, y))\n",
    "\n",
    "# Build arrays\n",
    "g_arr = np.array([r[0] for r in rows])\n",
    "t_arr = np.array([r[1] for r in rows], dtype=float)\n",
    "post  = np.array([r[2] for r in rows], dtype=float)\n",
    "y     = np.array([r[3] for r in rows], dtype=float)\n",
    "\n",
    "# Group fixed effects via dummies (use A as the reference)\n",
    "G_B = (g_arr == \"B\").astype(float)\n",
    "G_C = (g_arr == \"C\").astype(float)\n",
    "\n",
    "# --------------------------\n",
    "# 2) Model 1: Group FE + common trend + Post (Diff-in-Diff style with shared event)\n",
    "#     y_it = c + a_B*1{g=B} + a_C*1{g=C} + beta * t + gamma * Post_t + e_it\n",
    "# --------------------------\n",
    "X1 = np.column_stack([np.ones_like(y), G_B, G_C, t_arr, post])\n",
    "\n",
    "def ols_fit(y, X):\n",
    "    XtX = X.T @ X\n",
    "    XtX_inv = np.linalg.inv(XtX)\n",
    "    b = XtX_inv @ (X.T @ y)\n",
    "    resid = y - X @ b\n",
    "    n,k = X.shape\n",
    "    s2 = (resid @ resid) / (n - k)\n",
    "    V = s2 * XtX_inv\n",
    "    se = np.sqrt(np.diag(V))\n",
    "    return b, se\n",
    "\n",
    "if HAVE_SM:\n",
    "    m1 = sm.OLS(y, X1).fit()\n",
    "    b1 = m1.params; se1 = m1.bse\n",
    "else:\n",
    "    b1, se1 = ols_fit(y, X1)\n",
    "\n",
    "# --------------------------\n",
    "# 3) Model 2: Group FE + common trend + Event-Study (leads/lags around t=0)\n",
    "#     Build relative-time dummies D_k = 1{ t == k }, exclude k = -1 as baseline.\n",
    "#     Coefficients on post-event k >= 0 should be near the common jump delta.\n",
    "# --------------------------\n",
    "# Choose a symmetric window for the event study (you can widen this)\n",
    "K_pre = 5\n",
    "K_post = 5\n",
    "keep_mask = (t_arr >= -K_pre) & (t_arr <= K_post)\n",
    "# Filter to the window to keep matrix manageable and to avoid perfect collinearity with the trend\n",
    "y2 = y[keep_mask]; t2 = t_arr[keep_mask]; G_B2 = G_B[keep_mask]; G_C2 = G_C[keep_mask]\n",
    "\n",
    "# Build relative-time design (exclude k = -1 as reference)\n",
    "rel_ks = list(range(-K_pre, K_post+1))\n",
    "if -1 in rel_ks:\n",
    "    rel_ks.remove(-1)\n",
    "\n",
    "Dcols = []\n",
    "for k in rel_ks:\n",
    "    Dcols.append((t2 == k).astype(float))\n",
    "D = np.column_stack(Dcols) if len(Dcols) else np.empty((len(y2),0))\n",
    "\n",
    "# Final design: intercept + group FE + linear trend + relative-time dummies\n",
    "X2 = np.column_stack([np.ones_like(y2), G_B2, G_C2, t2, D])\n",
    "\n",
    "if HAVE_SM:\n",
    "    m2 = sm.OLS(y2, X2).fit()\n",
    "    b2 = m2.params; se2 = m2.bse\n",
    "else:\n",
    "    b2, se2 = ols_fit(y2, X2)\n",
    "\n",
    "# --------------------------\n",
    "# 4) Print concise results\n",
    "# --------------------------\n",
    "def fmt(x): return f\"{x: .3f}\"\n",
    "\n",
    "print(\"=== Model 1: Group FE + Trend + Post ===\")\n",
    "print(\"Columns: [Intercept, 1{B}, 1{C}, t, Post]\")\n",
    "print(\"Coef   :\", \"  \".join(map(fmt, b1)))\n",
    "print(\"SE     :\", \"  \".join(map(fmt, se1)))\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  â€¢ Coef(Post) â‰ˆ {fmt(b1[4])} (true jump Î´ = {delta}).\")\n",
    "print(f\"    This is the estimated **common event effect**, net of group fixed effects (baseline differences)\")\n",
    "print(f\"    and a shared time trend (parallel trends).\")\n",
    "print(f\"  â€¢ Coefs on 1{{B}}, 1{{C}} are **group fixed effects** capturing baseline level differences vs group A.\\n\")\n",
    "\n",
    "print(\"=== Model 2: Group FE + Trend + Event-Study Window ===\")\n",
    "print(\"Columns: [Intercept, 1{B}, 1{C}, t, D_k ...]  (with k=-1 omitted as baseline)\")\n",
    "post_positions = [i for i,k in enumerate(rel_ks) if k >= 0]\n",
    "pre_positions  = [i for i,k in enumerate(rel_ks) if k < 0]\n",
    "\n",
    "# Pull the first few event-time coefficients to show the pattern\n",
    "# They start at index 4 in b2 (after intercept, FE dummies, and trend)\n",
    "ev_coefs = b2[4:]\n",
    "print(\"Selected event-time coefficients (should be ~0 pre, ~Î´ post):\")\n",
    "preview = []\n",
    "for idx, k in enumerate(rel_ks):\n",
    "    preview.append(f\"k={k:>2}: {fmt(ev_coefs[idx])}\")\n",
    "print(\"  \" + \" | \".join(preview))\n",
    "\n",
    "# Quick check: average of post-event coefficients should be near delta\n",
    "if len(post_positions) > 0:\n",
    "    mean_post = np.mean([ev_coefs[i] for i in post_positions])\n",
    "    print(f\"\\nMean of post-event coefficients â‰ˆ {fmt(mean_post)}  (true Î´ = {delta})\")\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c05733",
   "metadata": {},
   "source": [
    "## âœ… What This Did & How the Model Works\n",
    "\n",
    "We constructed a dataset with **three groups (A, B, C)** over time. Each group:\n",
    "\n",
    "- Has a **different baseline level**  \n",
    "- Follows the **same linear (parallel) trend over time**  \n",
    "- Experiences a **shared event at time t = 0**, where all three groups **jump upward by the same amount**\n",
    "\n",
    "The data-generating process (DGP) is:\n",
    "\n",
    "$$\n",
    "y_{g,t} = \\alpha_g + \\beta \\cdot t + \\delta \\cdot \\mathbf{1}(t \\ge 0) + \\varepsilon_{g,t}\n",
    "$$\n",
    "\n",
    "- $\\alpha_g$ = group-specific baseline  \n",
    "- $\\beta$ = common linear trend (parallel trends assumption)  \n",
    "- $\\delta$ = effect of the event (same across groups)  \n",
    "- $\\mathbf{1}(t \\ge 0)$ = event indicator (0 before, 1 after)  \n",
    "- $\\varepsilon_{g,t} \\sim N(0, \\sigma^2)$  \n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Model 1 â€” Group Fixed Effects + Event Dummy (Difference-in-Differences style)\n",
    "\n",
    "This model detects a **single jump at the event** and adjusts for **baseline differences across groups**.\n",
    "\n",
    "$$\n",
    "y_{g,t}\n",
    "= c\n",
    "+ \\alpha_B \\cdot \\mathbf{1}(g=B)\n",
    "+ \\alpha_C \\cdot \\mathbf{1}(g=C)\n",
    "+ \\beta \\cdot t\n",
    "+ \\gamma \\cdot \\mathbf{1}(t \\ge 0)\n",
    "+ u_{g,t}\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- $(\\alpha_B, \\alpha_C)$ = **group fixed effects**, capturing baseline differences vs. Group A  \n",
    "- $\\beta \\cdot t$ = **shared linear trend across all groups**  \n",
    "- $\\gamma$ = **estimated common event effect** (should recover $\\delta$)  \n",
    "- This model only detects a **level change**, not slope or curvature changes \n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Model 2 â€” Group Fixed Effects + Event Study (Leads & Lags)\n",
    "\n",
    "Allows us to see how the effect evolves over time and tests for **pre-trends**.\n",
    "\n",
    "$$\n",
    "y_{g,t}\n",
    "= c\n",
    "+ \\alpha_B \\cdot \\mathbf{1}(g = B)\n",
    "+ \\alpha_C \\cdot \\mathbf{1}(g = C)\n",
    "+ \\beta \\cdot t\n",
    "+ \\sum_{k \\ne -1} \\theta_k \\cdot \\mathbf{1}(t = k)\n",
    "+ u_{g,t}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\theta_k$ = effect at time $k$ relative to the event  \n",
    "- We **omit $k = -1$** as the baseline period  \n",
    "- If the event is real:\n",
    "  - **Pre-event ($k < 0$):** $\\theta_k \\approx 0$  \n",
    "  - **Post-event ($k \\ge 0$):** $\\theta_k \\approx \\delta$ \n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Why This Works\n",
    "\n",
    "| What the Model Handles          | How It Is Captured                        |\n",
    "|----------------------------------|-------------------------------------------|\n",
    "| Baseline differences by group   | Group fixed effects (\\( \\alpha_g \\))      |\n",
    "| Shared time evolution           | Common time trend (\\( \\beta \\cdot t \\))   |\n",
    "| Sudden event shift              | \\( \\gamma \\) (Model 1) or \\( \\theta_k \\) (Model 2) |\n",
    "| Parallel trends assumption       | Same \\( \\beta \\) across groups            |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Summary\n",
    "\n",
    "âœ” Data follows **parallel trends** before the event  \n",
    "âœ” Groups differ only in **starting levels (fixed effects)**  \n",
    "âœ” All groups experience the **same jump at** $t = 0$  \n",
    "âœ” Model 1 estimates the **overall average event effect** ($\\gamma$)  \n",
    "âœ” Model 2 shows **dynamic effects over time** ($\\theta_k$) and checks assumptions (no pre-trend)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4194d2",
   "metadata": {},
   "source": [
    "# Week 12 Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4af6f8",
   "metadata": {},
   "source": [
    "**Construct a dataset in which prior trends do not hold, and in which this makes the differences-in-differences come out wrong. Explain why the differences-in-differences estimate of the effect comes out higher or lower than the actual effect.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fa012d",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Idea\n",
    "\n",
    "Create two groups, **Treated** and **Control**. Even **without** the treatment, Treated was already improving faster.  \n",
    "After the event (**Post**), Treated also receives a real treatment effect $\\tau$.  \n",
    "A standard differences-in-differences (DiD) estimator will then **overstate the effect** because it wrongly attributes this **pre-existing faster growth** to the policy.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Math: Why DiD Fails When Trends Arenâ€™t Parallel\n",
    "\n",
    "In a 2-period DiD setup, the estimator is:\n",
    "\n",
    "$$\n",
    "\\hat{\\tau}_{DiD}\n",
    "= \\underbrace{\\Delta \\bar{y}_{\\text{Treated}}}_{\\text{post-pre}}\n",
    "- \\underbrace{\\Delta \\bar{y}_{\\text{Control}}}_{\\text{post-pre}}\n",
    "= \\tau + (g_T - g_C),\n",
    "$$\n",
    "\n",
    "So the **bias** is:\n",
    "\n",
    "$$\n",
    "\\text{Bias}(\\hat{\\tau}_{DiD}) = g_T - g_C\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $g_T$ = natural (non-treatment) trend of the treated group  \n",
    "- $g_C$ = natural trend of the control group  \n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Interpretation\n",
    "\n",
    "- If **$g_T > g_C$** (treated was already trending up faster):  \n",
    "  â†’ DiD will **overestimate** the true treatment effect.\n",
    "\n",
    "- If **$g_T < g_C$** (treated trending down relative to control):  \n",
    "  â†’ DiD will **underestimate** (or even reverse sign) of the treatment effect.\n",
    "\n",
    "---\n",
    "\n",
    "This happens because DiD **assumes parallel trends**, i.e.:\n",
    "\n",
    "$$\n",
    "\\text{(Treated post âˆ’ Treated pre)} - \\text{(Control post âˆ’ Control pre)}\n",
    "$$\n",
    "\n",
    "should equal only the treatment effect.  \n",
    "But if groups were already moving differently before treatment, then DiD cannot separate **trend differences** from **policy effects**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49289a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell means:\n",
      "post          0       1\n",
      "group                  \n",
      "Control   9.963  10.284\n",
      "Treated  11.956  15.061 \n",
      "\n",
      "True effect Ï„ = 2.000\n",
      "DiD estimate  = 2.784\n",
      "Bias          = 0.784   (expected â‰ˆ g_T - g_C = 0.700)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Set up a non-parallel-trends DGP\n",
    "# -----------------------------\n",
    "N_per_group = 400          # individuals per group\n",
    "sigma = 1.0                # noise sd\n",
    "tau_true = 2.0             # true treatment effect (level jump at Post for Treated only)\n",
    "\n",
    "# Growth components (absent treatment)\n",
    "# Choose g_T > g_C to create positive bias (DiD overestimates).\n",
    "g_T = 1.0                  # treated group's baseline change from Pre->Post without treatment\n",
    "g_C = 0.3                  # control group's baseline change from Pre->Post\n",
    "\n",
    "# Two periods: 0 = Pre, 1 = Post (simplest DiD setup)\n",
    "periods = [0, 1]\n",
    "groups = [\"Control\", \"Treated\"]\n",
    "\n",
    "rows = []\n",
    "for g in groups:\n",
    "    for t in periods:\n",
    "        # baseline level per group at Pre (can differ)\n",
    "        alpha = 10.0 if g == \"Control\" else 12.0\n",
    "\n",
    "        # group-specific \"natural\" change from Pre to Post (violates parallel trends)\n",
    "        natural_change = g_C if g == \"Control\" else g_T\n",
    "        trend = 0.0 if t == 0 else natural_change\n",
    "\n",
    "        # treatment effect applies only to Treated in Post\n",
    "        treat = 1 if (g == \"Treated\" and t == 1) else 0\n",
    "        y_mean = alpha + trend + tau_true * treat\n",
    "\n",
    "        # draw individuals\n",
    "        y = y_mean + rng.normal(0, sigma, size=N_per_group)\n",
    "        for val in y:\n",
    "            rows.append((g, t, treat, val))\n",
    "\n",
    "df = pd.DataFrame(rows, columns=[\"group\", \"post\", \"treated_post\", \"y\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Compute the DiD estimator from cell means\n",
    "# -----------------------------\n",
    "# Mean outcomes by group & period\n",
    "means = df.groupby([\"group\", \"post\"])[\"y\"].mean().unstack()\n",
    "yC_pre, yC_post = means.loc[\"Control\", 0], means.loc[\"Control\", 1]\n",
    "yT_pre, yT_post = means.loc[\"Treated\", 0], means.loc[\"Treated\", 1]\n",
    "\n",
    "DiD_hat = (yT_post - yT_pre) - (yC_post - yC_pre)\n",
    "bias = DiD_hat - tau_true\n",
    "\n",
    "print(\"Cell means:\")\n",
    "print(means.round(3), \"\\n\")\n",
    "print(f\"True effect Ï„ = {tau_true:.3f}\")\n",
    "print(f\"DiD estimate  = {DiD_hat:.3f}\")\n",
    "print(f\"Bias          = {bias:.3f}   (expected â‰ˆ g_T - g_C = {g_T - g_C:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858959d",
   "metadata": {},
   "source": [
    "### âœ… What Youâ€™ll See (Typical Results)\n",
    "\n",
    "With the default settings in the simulation:\n",
    "\n",
    "- Treated group has a **faster underlying trend** than Control (violating parallel trends).  \n",
    "- The true treatment effect is:  \n",
    "  $$\n",
    "  \\tau = 2.0\n",
    "  $$\n",
    "- But the **DiD estimate will be larger than $\\tau$** because it wrongly attributes natural growth in the treated group to the treatment.\n",
    "\n",
    "### âœ… Why the Bias Happens\n",
    "\n",
    "Because the treated group was already improving faster by $(g_T - g_C)$ before the event, the DiD formula:\n",
    "\n",
    "$$\n",
    "\\hat{\\tau}_{DiD} \n",
    "= (\\bar{y}_{T,post} - \\bar{y}_{T,pre})\n",
    "- (\\bar{y}_{C,post} - \\bar{y}_{C,pre})\n",
    "$$\n",
    "\n",
    "actually becomes:\n",
    "\n",
    "$$\n",
    "\\hat{\\tau}_{DiD} = \\tau + (g_T - g_C)\n",
    "$$\n",
    "\n",
    "So the **sign and size of the bias depend on trend differences:**\n",
    "\n",
    "| Case | Interpretation | DiD Result |\n",
    "|------|----------------|------------|\n",
    "| $g_T > g_C$ | Treated group was already accelerating faster | **Overestimates** effect |\n",
    "| $g_T < g_C$ | Treated was declining or growing slower | **Underestimates** effect |\n",
    "| $g_T = g_C$ | Parallel trends hold | **Correctly estimates** Ï„ |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Key Takeaway\n",
    "\n",
    "Differences-in-differences works **only if groups would have followed the same trend without treatment**.\n",
    "\n",
    "When this assumption fails:\n",
    "\n",
    "- DiD confuses natural trend differences with treatment effects  \n",
    "- It can mislead policy evaluation â€” showing false positives or negatives  \n",
    "- This is why **checking pre-trends** (via event studies or plotting) is essential before trusting DiD results âœ…"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
